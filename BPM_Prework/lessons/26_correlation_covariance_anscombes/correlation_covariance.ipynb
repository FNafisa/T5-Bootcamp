{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ The purpose of this lecture is to understand correlation and covariance.\n",
    "\n",
    "__At the end of this lecture you will be able to:__\n",
    "> 1. Understand some summary statistics such as Correlation and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Correlation and Covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- We can use a series of statistics to evaluate the relationship between the two variables:\n",
    "> 1. __[Covariance](https://en.wikipedia.org/wiki/Covariance):__ Covariance measures how the two variables vary with each other. Covariance investigates if the two variables tend to increase and decrease together or if one variable increases when the other decreases or vice versa or if the variables do not vary at all with each other (covariance of 0)\n",
    ">> - The formula for Covariance can be represented in terms of Expected Value and Variance: \n",
    "<center> $Cov(X, Y) = \\sigma_{XY} = E[XY] - E[X][Y] = E[(X - \\mu_{X})(Y - \\mu_{Y})]$ </center> \n",
    "\n",
    ">> - The formula for Covariance can also be represented in the following way:\n",
    "<center> $Cov(X, Y) = \\sigma_{XY} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\mu_{x})(y_{i} - \\mu_{y})}{n}$ </center>\n",
    "\n",
    ">> - If two variables $X$ and $Y$ are independent, then $Cov(X,Y) = 0$, but the opposite is not always true \n",
    ">> - The problem with Covariance is that it is not normalized and therefore difficult to determine if the magnitude of Covariance is considered strong or weak. This warrants a normalized measure of variable association of which we define Correlation below \n",
    "> 2. __[Correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence):__ Correlation is very similar to Covariance but is scaled by the Standard Deviations of the two variables such that Correlation can only range between -1 and +1. \n",
    ">> - The formula for Correlation can be represented in the following way:\n",
    "<center> $Cor(X, Y) = r_{XY} = \\frac{\\frac{\\sum_{i=1}^{n} (x_{i} - \\mu_{x})(y_{i} - \\mu_{y})}{n}}{\\sigma_x \\sigma_y}$ </center> \n",
    "\n",
    ">> - We can now interpret the mangitude of the Correlation and conclude that if the Correlation is positive, the variables move in the same direction, if the Correlation is negative is negative, the two variables move in opposite direction, and if the Correlation is 0, the variables do move together in either direction \n",
    "\n",
    "__Helpful Points:__\n",
    "1. It is possible to create Covariance and Correlation Matrices which calculates the Covariance and Correlation, respectively, of a series of variables and summarizes this information into a symmetric matrix\n",
    "2. Typically, we calculate the Covariance and Correlation Matrices of a data set to observe all the paired covariance and correlation values for every pair of variables \n",
    "\n",
    "__Practice:__ Examples of Correlation and Covariance in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data to analyze \n",
    "nba_df = pd.read_csv(\"NBA_GameLog_2010_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 (Covariance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of nba data for cov and cor calculations \n",
    "nba_df_subset = nba_df.loc[:, ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_df_subset_cov = pd.DataFrame(np.cov(nba_df_subset.T)) # cov function calculates row based covariances by default, but we want column covariances\n",
    "nba_df_subset_cov.columns = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_cov.index = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the covariance between every pair of variables in the subsetted data set. For example:\n",
    "- The covariance between Team Points and Team Total Rebounds is 10.05\n",
    "- The covariance between Team 3 Point Shot Percentage and Team Steal sis -0.003\n",
    "\n",
    "Note:\n",
    "1. The matrix is symmetric so you only need to consider either the upper right triangle or the lower left triangle \n",
    "2. The diagonal elements represent the covariance of the variable onto itself which is just the variance of that variable (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variance of each column which is the diagonal elements of the covariance matrix \n",
    "nba_df_subset.var(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 (Correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_df_subset_corr = pd.DataFrame(np.corrcoef(nba_df_subset.T)) # corrcoef function calculates row based correlations by default, but we want column correlations\n",
    "nba_df_subset_corr.columns = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_corr.index = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the correlation between every pair of variables in the subsetted data set. For example:\n",
    "- The correlation between Team Points and Team Total Rebounds is 0.127\n",
    "- The correlation between Team 3 Point Shot Percentage and Team Steal is -0.01\n",
    "\n",
    "Note:\n",
    "1. The matrix is symmetric so you only need to consider either the upper right triangle or the lower left triangle \n",
    "2. The diagonal elements represent the correlation of the variable onto itself which is perfect correlation (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to analyze graphically the relationship between __Tm.Pts__ and __Tm.FG_Perc__ which are highly correlated, as well as __Tm.3P_Perc__ and __Tm.TOV__ which have a low correlation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will normalize the features (calculate their z-score) by subtracting their mean and dividing by their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pts_Norm = (nba_df_subset['Tm.Pts']-np.mean(nba_df_subset['Tm.Pts']))/np.std(nba_df_subset['Tm.Pts'])\n",
    "FG_Perc_Norm = (nba_df_subset['Tm.FG_Perc']-np.mean(nba_df_subset['Tm.FG_Perc']))/np.std(nba_df_subset['Tm.FG_Perc'])\n",
    "\n",
    "ThreeP_Perc_Norm = (nba_df_subset['Tm.3P_Perc']-np.mean(nba_df_subset['Tm.3P_Perc']))/np.std(nba_df_subset['Tm.3P_Perc'])\n",
    "TOV_Norm = (nba_df_subset['Tm.TOV']-np.mean(nba_df_subset['Tm.TOV']))/np.std(nba_df_subset['Tm.TOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(nba_df_subset['Tm.Pts'])\n",
    "plt.plot(nba_df_subset['Tm.FG_Perc'])\n",
    "plt.legend(['Pts','FG_Perc'])\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Pts_Norm,alpha = 0.5)\n",
    "plt.plot(FG_Perc_Norm,alpha = 0.5)\n",
    "plt.legend(['Normalized Pts','Normalized FG_Perc'])\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(Pts_Norm,FG_Perc_Norm,alpha = 0.1)\n",
    "plt.xlabel('Normalized Pts')\n",
    "plt.ylabel('Normalized FG_Perc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a relationship between 'Pts' and 'FG_Perc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(nba_df_subset['Tm.3P_Perc'])\n",
    "plt.plot(nba_df_subset['Tm.TOV'])\n",
    "plt.legend(['3P_Perc','TOV'])\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(ThreeP_Perc_Norm,alpha = 0.5)\n",
    "plt.plot(TOV_Norm,alpha = 0.5)\n",
    "plt.legend(['Normalized ThreeP_Perc','Normalized TOV'])\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(ThreeP_Perc_Norm,TOV_Norm,alpha = 0.1)\n",
    "plt.xlabel('Normalized ThreeP_Perc')\n",
    "plt.ylabel('Normalized TOV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is a relationship between '3P_Perc' and 'TOV'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Select the quantitative variables from the Seattle Home Price data and develop both a Covariance and Correlation Matrix as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df = pd.read_csv(\"SeattleHomePrices.csv\")\n",
    "# Write your code here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 (Correlation Heat Map):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.corrcoef(nba_df_subset.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Select the quantitative variables from the Seattle Home Price data and develop both a Covariance and Correlation Matrix as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df = pd.read_csv(\"SeattleHomePrices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset = home_df.loc[:, ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset = home_df_subset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset_cov = pd.DataFrame(np.cov(home_df_subset.T))\n",
    "home_df_subset_cov.columns = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_cov.index = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset_corr = pd.DataFrame(np.corrcoef(home_df_subset.T))\n",
    "home_df_subset_corr.columns = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_corr.index = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
