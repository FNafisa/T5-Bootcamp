{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Scraping\n",
    "\n",
    "_Tips and tricks for scraping the unscrapable._\n",
    "\n",
    "### Installation\n",
    "\n",
    "If you'd like to create a random user agent when scraping, install `fake_useragent`:\n",
    "\n",
    "With pip<br>\n",
    "`pip install fake_useragent`\n",
    "\n",
    "Or with conda<br>\n",
    "`conda install -c mlgill fake-useragent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much is too much?\n",
    "\n",
    "Sites have `robots.txt` pages that give guidelines about what they want to allow webcrawlers to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.github.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very end of this file, you will see:\n",
    "> `User-agent: *` <br>\n",
    "> `Disallow: /`\n",
    "\n",
    "This means disallow everything to all user-agents not previously covered... that's us!\n",
    "\n",
    "Boxofficemojo is much more accepting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.boxofficemojo.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How often is too often?\n",
    "\n",
    "It is very common for sites to block you if you send too many requests in a certain time period. Sometimes you can get around this by adding well-designed pauses in your scraping. \n",
    "\n",
    "Options include:\n",
    "* pause after every request\n",
    "* pause after each `n` requests\n",
    "* pause at random intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short pause after every request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every request\n",
    "import time\n",
    "\n",
    "page_list = ['page1','page2','page3']\n",
    "\n",
    "for page in page_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longer pause after 200 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "page_list = ['page1','page2','page3','page4','page5','page6']\n",
    "\n",
    "for i, page in enumerate(page_list):\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    if (i+1 % 200 == 0):\n",
    "        time.sleep(320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random pause after each page (more human-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for page in page_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make requests look like a real browser?\n",
    "\n",
    "In a human controlled browser, user agent information lets the web server know your browser specifications so the server can match to your software's capabilities.  \n",
    "\n",
    "When making an automated request, you can specify your user agent to make your request look more authentic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com'\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "response  = requests.get(url, headers = user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even generate a random user agent to send."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)\n",
    "\n",
    "response  = requests.get(url, headers = user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the next user agent generated has different specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis_2] *",
   "language": "python",
   "name": "conda-env-metis_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
