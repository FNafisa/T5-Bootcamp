{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "- Review of how linear regression models work (if needed)\n",
    "- Understand the five major assumptions of linear regression\n",
    "- Analyze data to determine if and \"by how much\" these assumptions are violated \n",
    "- Employ fixes to data and/or models to improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Assumptions of Ordinary Least Squares\n",
    "\n",
    "We will use this [Duke Resource](http://people.duke.edu/~rnau/testing.htm) as a guide for the lab.\n",
    "\n",
    "> Note: Duke has the rules in four parts, and other sources can have up to seven rules. Authors can be a bit arbitrary in how they decide to group similar concepts.\n",
    "\n",
    "1. Regression is linear in parameters & correctly specified (Duke i)\n",
    "2. The error terms are normally distributed and zero population mean (Duke iv)\n",
    "3. The error term has constant variance $Var({\\epsilon_i})={\\sigma^2}$ for every i (Duke iii, no heteroskedasticity)\n",
    "4. Errors are uncorrelated across observations: $cov({\\epsilon_i},{\\epsilon_j})=0$ for two observations i and j (Duke ii, no serial correlation)\n",
    "5. No independent variable is a perfect linear function of any other independent variable (Duke i, no perfect multi-collinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "import scipy.stats as stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 1: regression is linear in parameters and correctly specified\n",
    "\n",
    "This is linear: $ Y= {\\beta_0}+ {\\beta_1}X_1+{\\beta_2}X_2 +{\\epsilon}$\n",
    "\n",
    "This is not: $ Y= {\\beta_0}+ e^{\\beta_1}X^{\\beta_2}$\n",
    "\n",
    "Notice we're not talking about straight lines vs. curved (quadratic or other powers). Rather, the second equation doesn't have scalar coefficients; they change X in strange ways depending on where you are on the axis. Also, we _cannot_ use linear algebra (matrix multiplication) to solve our model for an optimal solution. These 'non-linear models' are a whole different - much more complex - domain of modeling, and outside the scope of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plot(x, y):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    \n",
    "    rgr = LinearRegression()\n",
    "    rgr.fit(x,y)\n",
    "    pred = rgr.predict(x)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(x, pred, color='blue',linewidth=1)\n",
    "    plt.title(\"Regression fit\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    res = y - pred\n",
    "    plt.scatter(pred, res)\n",
    "    plt.title(\"Residual plot\")\n",
    "    plt.xlabel(\"prediction\")\n",
    "    plt.ylabel(\"residuals\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    #Generates a probability plot of sample data against the quantiles of a \n",
    "    # specified theoretical distribution \n",
    "    stats.probplot(res, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Normal Q-Q plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake data\n",
    "s = 500\n",
    "x = np.random.uniform(low=-5, high=5, size=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 2*np.random.randn(s)\n",
    "beta = 2\n",
    "y = beta*x + epsilon\n",
    "\n",
    "diagnostic_plot(x.reshape(s,1), y) # we reshape x to turn it into a tall column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose:\n",
    "\n",
    "Regression fit: Inspect plot of observed data vs predicted values (points should be symmetric around the line).\n",
    "\n",
    "Residual plot: Points should be symmetric around y=0 with roughly constant variance. Look carefully for evidence of a \"bowed\" pattern, indicating that the model makes systematic errors whenever it is making unusually large or small predictions.\n",
    "\n",
    "Q-Q plot: Look for the middle section of dots to be very close to the diagonal red line. Use the chart below as a reference for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/ZXRkL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 1\n",
    "y = beta1*(x**2) + epsilon\n",
    "\n",
    "diagnostic_plot(x.reshape(s,1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of broken assumptions for linear models. The model cannot correctly summarize the underlying relationship. If this is true for your data, among other recommendations from Duke and the Additional Resources listed at bottom, you can transform your data to make it more linear using log-transformations or other means, or you may have to utilize a non-linear model on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 2: residuals ( ${e_i} = Y_i-\\hat{Y}_i$ ) should be normally distributed with zero mean\n",
    "\n",
    "We can check this assumption on a real bikeshare dataset by plotting our residuals vs $\\hat{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore our assumptions - Let's look at some bike data\n",
    "data = pd.read_csv('./data/hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-numeric data\n",
    "cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed']\n",
    "X = data[cols]\n",
    "y = data.cnt   # predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop OLS with Sklearn\n",
    "lr = LinearRegression()\n",
    "fit = lr.fit(X,y) # for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your predicted values on the x-axis, and your residuals on the y-axis\n",
    "\n",
    "data['predict']=fit.predict(X)\n",
    "data['resid']=data.cnt-data.predict\n",
    "with sns.axes_style('white'):\n",
    "    plot=data.plot(kind='scatter',\n",
    "                  x='predict',y='resid',alpha=0.2,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: What is going on with the lower bound of the plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect histogram\n",
    "data.cnt.hist(bins=35)\n",
    "plt.title('Histogram of Dependent Variable (User Counts)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose/inspect residual normality using qqplot:\n",
    "stats.probplot(data['resid'], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer:\n",
    "\n",
    "> Note the diagonal pattern in the residuals... the target for our bike dataset is a count of riders, which is NOT continuous: it cannot be negative and must be an integer. It's count data! There is not a truly linear relationship between features and target!\n",
    "\n",
    "> Therefore, the assumption that errors are normally distributed can't be held (more about count data when we discuss the poisson distribution in a couple of weeks). Remember that our dependent variable should not be categorical or ordinal (i.e. rank of films) either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 3: error terms must have constant variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition to movie data to see a violation of assumption 3\n",
    "movie=pd.read_csv('./data/2013_movies.csv')\n",
    "\n",
    "# drop NaNs for quick analysis\n",
    "movie=movie.dropna()\n",
    "movie.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at numeric data\n",
    "X = movie[['Budget','Runtime']]\n",
    "y = movie.DomesticTotalGross\n",
    "\n",
    "model = sm.OLS(y,X)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the helpful warnings in `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create residual plot\n",
    "movie['predict']=fit.predict(X)\n",
    "movie['resid']= y-movie.predict\n",
    "with sns.axes_style('white'):\n",
    "    plot = movie.plot(\n",
    "        kind='scatter', x='predict', y='resid', alpha=0.5, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q: What is wrong with the above plot? Which assumption is this plot inconsistent with? What can we do about it?  \n",
    "\n",
    ">A: This plot is inconsistent with **Assumption #3**: (The error term must have constant variance).\n",
    "\n",
    "We see signs of heteroskedasticity; however, the rule of thumb is OLS regression isn't too impacted by heteroscedasticity as long as the maximum variance is not greater than **four** times the minimum variance (as in this case using simple height comparisons across the x-axis). If the residual variance of your model exceeds this range, we can opt for a Weighted Least Squares (WLS) model.\n",
    "\n",
    "Here's an additional resource on [building WLS models in Python](https://towardsdatascience.com/when-and-how-to-use-weighted-least-squares-wls-models-a68808b1a89d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram\n",
    "movie.DomesticTotalGross.hist();\n",
    "\n",
    "# note the positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick reg plot\n",
    "plt.scatter(movie.Budget,y)\n",
    "plt.scatter(movie.Budget,movie.predict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are in orange, and they're not able to capture the exponential explosions of some movie profits. We can compress the raw data and see if that helps us model more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log transformation\n",
    "np.log(movie.DomesticTotalGross).hist();\n",
    "\n",
    "# it looks better but not very \"normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try another transformation!\n",
    "Examples of [box-cox in action](http://scikit-learn.org/dev/auto_examples/preprocessing/plot_power_transformer.html) (sklearn calls it power transform) on various distributions.\n",
    "\n",
    "Note the [scipy transform formula](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=stats.boxcox_normmax(movie.DomesticTotalGross, brack=(-1.9, 1.9)) # don't use \"lambda\" as it's a Python reserved word\n",
    "print(\"Lambda:\", lamb)\n",
    "y_t=(np.power(movie.DomesticTotalGross,-0.2282)-1)/-0.2282\n",
    "\n",
    "plt.hist(y_t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show optimal lambda values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "prob = stats.boxcox_normplot(movie.DomesticTotalGross, -3, 5, plot=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note above that `y` peaks at lambda = -.22\n",
    "\n",
    "It's not perfect... Box-cox can't fix every problem but it will improve things for badly skewed data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On your own: \n",
    "- Use box-cox transformation to also transform Budget & Runtime\n",
    "- Visualize residuals on a new model with all features transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 4: errors are uncorrelated across observations\n",
    "\n",
    "To check this assumption, let's plot residuals vs. time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine date and time into a single datetime column\n",
    "movie[\"DATE_TIME\"] = pd.to_datetime(movie.ReleaseDate , format=\"%Y-%m-%d\")\n",
    "movie.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = movie[['DATE_TIME','resid']].set_index('DATE_TIME')\n",
    "ts.plot(style=\".\");\n",
    "\n",
    "# there seems to be no pattern, so we're good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix: If we did see a strong relationship between errors and time, this would indicate that our model is incorrectly specified. We might apply a time-series model if there's natural [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) (a concept introduced later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 5: no independent variable is a perfect linear function of any other independent variable (no perfect multi-collinearity)\n",
    "\n",
    "Ways to diagnose: \n",
    "1. Inspect correlations of independent features\n",
    "2. Keep an eye on condition number!\n",
    "    3. As noted above, `statsmodels` will notify you of [large condition numbers](https://en.wikipedia.org/wiki/Condition_number).\n",
    "\n",
    "Ways to fix:\n",
    "3. Consider Partial Least Squares or projection into latent space (PCA, introduced in the second-half of the course)\n",
    "4. Use Ridge regularization\n",
    "\n",
    "Incorporating [Ridge regularization](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) not only fixes this issue (if it exists, still safe to use if it doesn't), but it imparts other benefits as well. We'll use Ridge quite often after it's officially introduced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "- Review of how linear regression models work (if needed)\n",
    "- Understand the five major assumptions of linear regression\n",
    "- Analyze data to determine if and \"by how much\" these assumptions are violated \n",
    "- Employ fixes to data and/or models to improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Duke Guide](http://people.duke.edu/~rnau/testing.htm)\n",
    "- [10 Assumptions List](http://r-statistics.co/Assumptions-of-Linear-Regression.html)\n",
    "- [University of Wisconsin List](http://blog.uwgb.edu/bansalg/statistics-data-analytics/linear-regression/what-are-the-four-assumptions-of-linear-regression/)\n",
    "- [Statistics Solutions Guide](http://www.statisticssolutions.com/assumptions-of-linear-regression/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis_2] *",
   "language": "python",
   "name": "conda-env-metis_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
