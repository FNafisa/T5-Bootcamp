{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Linear Regression: Applied to the Ames Housing Data\n",
    "\n",
    "Using the Ames Housing Data:\n",
    "\n",
    "Dean De Cock\n",
    "Truman State University\n",
    "Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "\n",
    "In this notebook, we will build some linear regression models to predict housing prices from this data. In particular, we will set out to improve on a baseline set of features via **feature engineering**: deriving new features from our existing data. Feature engineering is done extensively in machine learning applications, and it often makes the difference between a weak or decent model and a strong one. Sometimes feature engineering is even more important than model selection and hyperparameter tuning when it comes to improving predictions!      \n",
    "\n",
    "We will split our data into training and validation sets, build models on various feature sets and compare their results on the validation set. We will use visual exploration, domain understanding, and intuition to construct new features that add significant predictive signal, which will be apparent in validation *r-squared* scores.\n",
    "\n",
    "**Notebook Contents**\n",
    "\n",
    "> 1. Simple EDA and baseline model\n",
    "> 2. Basic feature engineering: adding polynomial terms\n",
    "> 3. Basic feature engineering: adding interaction terms\n",
    "> 4. Intermediate feature engineering: categories and features derived from category aggregates \n",
    "\n",
    "## 1. Simple EDA and Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data, Examine and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the Ames Housing Data\n",
    "datafile = \"data/Ames_Housing_Data.tsv\"\n",
    "df=pd.read_csv(datafile, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the columns, look at missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is recommended by the data set author to remove a few outliers\n",
    "\n",
    "df = df.loc[df['Gr Liv Area'] <= 4000,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a *lot* of variables, many of which have a lot of missing values.  Let's pick out just a few columns and start building models using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appears to be one NA in Garage Cars - fill with 0\n",
    "smaller_df = smaller_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice, filtered dataset, let's generate visuals to better understand the target and feature-target relationships: pairplot is great for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df[:1000], plot_kws=dict(alpha=.1, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Data Exploration Exercises**: \n",
    "\n",
    "1. What do these plots tell us about the distribution of the target?   \n",
    "\n",
    "2. What do these plots tell us about the relationship between the features and the target? Do you think that linear regression is well-suited to this problem? Do any feature transformations come to mind?\n",
    "\n",
    "3. What do these plots tell us about the relationship between various pairs of features? Do you think there may be any problems here? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for modeling and building a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate our features from our target\n",
    "\n",
    "X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars']]\n",
    "\n",
    "y = smaller_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have feature/target data X, y ready to go, we're nearly ready to fit and evaluate a baseline model using our current feature set. We'll need to create a **train/validation split** before we fit and score the model. \n",
    "\n",
    "Since we'll be repeatedly splitting X, y into the same train/val partitions and fitting/scoring new models as we update our feature set, we'll define a reusable function that completes all these steps, making our code/process more efficient going forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_validate(X, y):\n",
    "    '''\n",
    "    For a set of features and target X, y, perform a 80/20 train/val split, \n",
    "    fit and validate a linear regression model, and report results\n",
    "    '''\n",
    "    \n",
    "    # perform train/val split\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # fit linear regression to training data\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # score fit model on validation data\n",
    "    val_score = lr_model.score(X_val, y_val)\n",
    "    \n",
    "    # report results\n",
    "    print('\\nValidation R^2 score was:', val_score)\n",
    "    print('Feature coefficient results: \\n')\n",
    "    for feature, coef in zip(X.columns, lr_model.coef_):\n",
    "        print(feature, ':', f'{coef:.2f}') \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's go ahead and run this function on our baseline feature set and take some time to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_validate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Review Exercise**: How is the R^2 score defined? What does this score mean intuitively? What are possible drawbacks of using the R^2 score for evaluation? \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic feature engineering: adding polynomial terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things that we looked for in the pairplot was evidence about the relationship between each feature and the target. In certain features like _'Overall Qual'_ and _'Gr Liv Qual'_, we notice an upward-curved relationship rather than a simple linear correspondence. This suggests that we should add quadratic **polynomial terms or transformations** for those features, allowing us to express that non-linear relationship while still using linear regression as our model.\n",
    "\n",
    "Luckily, pandas makes it quite easy to quickly add those square terms as additional features to our original feature set. We'll do so and evaluate our model again below.\n",
    "\n",
    "As we add to our baseline set of features, we'll create a copy of the latest benchmark so that we can continue to store our older feature sets. **Note that you should be very careful about this in practice**, as it means that we are saving several redundant copies of the data: it's often better to continuously override the original feature set in order to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X.copy()\n",
    "\n",
    "X2['OQ2'] = X2['Overall Qual'] ** 2\n",
    "X2['GLA2'] = X2['Gr Liv Area'] ** 2\n",
    "\n",
    "split_and_validate(X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, adding appropriate square terms allows our model to do a significantly better job (+.05 R^2) capturing certain feature-target relationships that are closer to quadratic than linear. If we saw higher-order curve relationships, we could try adding higher degree polynomial terms as well.\n",
    "\n",
    "**Note**: feature transformations are not limited to polynomial terms and can also include log and square root transforms among others. Follow your instinct based on what you see in feature-target plots, and validate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Polynomial Feature Exercise**: Based on inspecting the pairplot, what other features do you think polynomial terms may be helpful for? Try adding them to X2 and rerunning `split_and_validate`. Is the improvement less than you expected? What do you think happened?  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic feature engineering: adding interaction terms\n",
    "\n",
    "With our current feature set, each feature value has no influence over how the model views other features' values. Each feature is treated as a completely independent quantity. However, there may easily be **interaction effects** present, in which the impact of one feature may dependent on the current value of a different feature.\n",
    "\n",
    "For example, there may be a higher premium for increasing _'Overall Qual'_ for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies _'Overall Qual'_ by _'Year Built'_ can help us capture it.\n",
    "\n",
    "Another style of interaction term involves feature proprtions: for example, to get at something like quality per square foot we could divide _'Overall Qual'_ by _'Lot Area'_.\n",
    "\n",
    "Let's try adding both of these interaction terms and see how they impact the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = X2.copy()\n",
    "\n",
    "# multiplicative interaction\n",
    "X3['OQ_x_YB'] = X3['Overall Qual'] * X3['Year Built']\n",
    "\n",
    "# division interaction\n",
    "X3['OQ_/_LA'] = X3['Overall Qual'] / X3['Lot Area']\n",
    "\n",
    "split_and_validate(X3, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, they gave us an additional boost of .01 R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Interaction Feature Exercise**: What other interactions do you think might be helpful? Try adding them to X3 and measuring your results!  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intermediate feature engineering: categories and features derived from category aggregates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating **categorical features** into linear regression models is fairly straightforward: we can create a new feature column for each category value, and fill these columns with 1s and 0s to indicate which category is present for each row. This method is called **dummy variables** or **one-hot-encoding**.\n",
    "\n",
    "We'll first explore this using the _'House Style'_ feature from the original dataframe. Before going straight to dummy variables, it's a good idea to check category counts to make sure all categories have reasonable representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['House Style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ok, and here's a quick look at how dummy features actually appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['House Style']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `pd.get_dummies()` on our entire dataset to quickly get data with all the original features and dummy variable representation of any categorical features. Let's do that below and run a new benchmark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = X3.copy()\n",
    "\n",
    "X4['House Style'] = df['House Style']\n",
    "\n",
    "split_and_validate(pd.get_dummies(X4), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we gained another .01 R^2. Let's think about bringing in another categorical variable, _Neighborhood_, from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_counts = df.Neighborhood.value_counts()\n",
    "nbh_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this category, let's map the few least-represented neighborhoods to an \"other\" category before adding the feature to our feature set and running a new benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_nbhs = list(nbh_counts[nbh_counts <= 8].index)\n",
    "\n",
    "X5 = X4.copy()\n",
    "\n",
    "X5['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')\n",
    "\n",
    "split_and_validate(pd.get_dummies(X5), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, another ~.01 R^2 boost, though at the expense of adding a large number of feature columns!\n",
    "\n",
    "#### Getting to fancier features\n",
    "\n",
    "Let's close out our introduction to feature engineering by considering a more complex type of feature that may work very nicely for certain problems. It doesn't seem to add a great deal over what we have so far, but it's a style of engineering to keep in mind for the future.\n",
    "\n",
    "We'll create features that capture where a feature value lies relative to the members of a category it belongs to. In particular, we'll calculate deviance of a row's feature value from the mean value of the category that row belongs to. This helps to capture information about a feature relative to the category's distribution, e.g. how nice a house is relative to other houses in its neighborhood or of its style.\n",
    "\n",
    "Below we define reusable code for generating features of this form, feel free to repurpose it for future feature engineering work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_deviation_feature(X, feature, category):\n",
    "    \n",
    "    # temp groupby object\n",
    "    category_gb = X.groupby(category)[feature]\n",
    "    \n",
    "    # create columns of category means and standard deviations\n",
    "    category_mean = category_gb.transform(lambda x: x.mean())\n",
    "    category_std = category_gb.transform(lambda x: x.std())\n",
    "    \n",
    "    # compute stds from category mean for each feature value,\n",
    "    # add to X as new feature\n",
    "    deviation_feature = (X[feature] - category_mean) / category_std \n",
    "    X[feature + '_Dev_' + category] = deviation_feature  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use our feature generation code to add 2 new deviation features, and run a final benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = X5.copy()\n",
    "\n",
    "add_deviation_feature(X6, 'Year Built', 'House Style')\n",
    "add_deviation_feature(X6, 'Overall Qual', 'Neighborhood')\n",
    "\n",
    "split_and_validate(pd.get_dummies(X6), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it didn't help much, but it did help a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Recap\n",
    "\n",
    "**Benchmarks**:\n",
    "\n",
    "> 1. Baseline feature set: ~.82 R^2 \n",
    "> 2. Add Several polynomial transforms: ~.87 R^2\n",
    "> 3. Add Several interaction terms: ~.88 R^2\n",
    "> 4. Add Category features (house style): ~.89 R^2\n",
    "> 5. Add Category features (neighborhood): ~.90 R^2\n",
    "> 6. Add Category deviation features: ~.901 R^2\n",
    "\n",
    "As you can see, feature engineering often follows a sort of [_Pareto principle_](https://en.wikipedia.org/wiki/Pareto_principle), where a large bulk of the predictive gains can be reached through adding a set of intuitive, strong features like polynomial transforms and interactions. Directly incorporating additional information like categorical variables can also be very helpful. Beyond this point, additional feature engineering can provide significant, but potentially diminishing returns. Whether it's worth making the extra effort is very dependent on the use case for the model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Further Practice Exercise**: Gamify this to practice your feature engineering skills! How far can you push the validation R^2 score with\n",
    "the features we've used so far? Can you bring in new features from the original dataframe to boost the score even more? \n",
    "\n",
    "-----\n",
    "\n",
    "-----\n",
    "**Reflection Question**: What problems do you foresee with highly extensive feature engineering? Is the model still easy to interpret and explain? What issues might arise if we were working with a massive dataset (say 100 million+ records) instead of only a few thousand?   \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis_2] *",
   "language": "python",
   "name": "conda-env-metis_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
